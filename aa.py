import torch
from copy import deepcopy
from setattn.custom_linear_attn import SetAttention_Linear_fla, SetAttention_Linear_fla_fast
# å‡è®¾è¿™ä¸¤ä¸ªç±»å·²ç»å®šä¹‰åœ¨å½“å‰ä½œç”¨åŸŸä¸­
# from your_module import SetAttention_Linear_fla, SetAttention_Linear_fla_fast

# === æ„é€ ä¸€ä¸ªç®€æ˜“çš„ config ===
class DummyConfig:
    n_embd = 64
    n_head = 4
    dropout = 0.0
    bias = False
    level = 2
    levelrand = False
    k_mapping = True
    v_mapping = False
    smaller_sets = False
    feature_map = "elu"

# åˆå§‹åŒ–
config = DummyConfig()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ä¸¤ä¸ªæ¨¡å‹
model_ref = SetAttention_Linear_fla(config).to(device)
model_fast = SetAttention_Linear_fla_fast(config).to(device)

# åŒæ­¥å‚æ•°ï¼ˆä¿è¯å®Œå…¨ä¸€è‡´ï¼‰
model_fast.load_state_dict(deepcopy(model_ref.state_dict()))

# === æ„é€ è¾“å…¥ ===
B, T, C = 4, 128, config.n_embd
torch.manual_seed(42)
x = torch.randn(B, T, C, device=device)

import time
# === å‰å‘è®¡ç®— ===
with torch.no_grad():
    start = time.time()
    for _ in range(10):
        out_ref = model_ref(x)
    end = time.time()
    print(f"ref time: {end - start:.6f}s")
    start = time.time()
    for _ in range(10):
        out_fast = model_fast(x)
    end = time.time()
    print(f"fast time: {end - start:.6f}s")

# === æ£€æŸ¥å½¢çŠ¶ ===
print(f"ref shape:  {out_ref.shape}")
print(f"fast shape: {out_fast.shape}")

# === æ•°å€¼è¯¯å·®åˆ†æ ===
abs_diff = (out_ref - out_fast).abs()
max_err = abs_diff.max().item()
mean_err = abs_diff.mean().item()

print(f"\nâœ… æœ€å¤§è¯¯å·®: {max_err:.6e}")
print(f"âœ… å¹³å‡è¯¯å·®: {mean_err:.6e}")

# === åˆ¤æ–­æ˜¯å¦ç­‰ä»· ===
if torch.allclose(out_ref, out_fast, atol=1e-5, rtol=1e-3):
    print("ğŸ¯ ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºä¸€è‡´ï¼")
else:
    print("âš ï¸ è¾“å‡ºå­˜åœ¨å·®å¼‚ï¼Œè¯·æ£€æŸ¥ä¸­é—´è®¡ç®—ã€‚")

# === é€‰å¡«ï¼šæ‰“å°å‡ ä¸ªç¤ºä¾‹å¯¹æ¯” ===
print("\nå‰5ä¸ªå…ƒç´ å¯¹æ¯”ï¼š")
print("ref:", out_ref.view(-1)[:5])
print("fast:", out_fast.view(-1)[:5])