# conf/config.yaml
out_dir: out
eval_interval: 2000
log_interval: 1
eval_iters: 200
eval_only: false
always_save_checkpoint: true
init_from: scratch

wandb:
  log: false
  project: owt
  run_name: gpt2

data:
  dataset: openwebtext
  gradient_accumulation_steps: 40
  batch_size: 12
  loss_type: cross_entropy

model:
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.1
  bias: false
  block_size: 1024
  post_LN: False
  pos_enc_type: nope  # 'learned', 'rope', 'alibi', 't5', 'nope'
  relative_attention_num_buckets: 32
  relative_attention_max_distance: 128

optim:
  learning_rate: 6e-4
  max_iters: 600000
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0

lr_decay:
  enabled: true
  warmup_iters: 2000
  lr_decay_iters: 600000
  min_lr: 6e-5

ddp:
  backend: nccl

system:
  device: cuda
  dtype: bfloat16
  compile: false

attn:
  type: vanilla
  set_policy: null
  set_aggr: null
  set_number: null
  feature_map: null
  k_mapping: True
  v_mapping: True
